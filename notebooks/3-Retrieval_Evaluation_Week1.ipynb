{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d49f27a-15ba-4176-bf6d-211051a8cef6",
   "metadata": {},
   "source": [
    "#### Week 1: Building Advanced RAG Applications.  Authored by Chris Sanchez."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a48d4e-11e4-45b5-8cfc-f595d61e11bd",
   "metadata": {},
   "source": [
    "# Week 1 - Notebook 3\n",
    "\n",
    "# Overview\n",
    "***\n",
    "Welcome to the final notebook for Week 1! Take a look at all the ground we've covered so far:\n",
    "- Chunking/splitting\n",
    "- Text vectorization\n",
    "- Combining metadata\n",
    "- Collection Configuration\n",
    "- Data Indexing\n",
    "- Keyword search\n",
    "- Vector search\n",
    "- OPTIONAL: Searching with Filters\n",
    "\n",
    "We are now prepared to move on to a very important topic, **Retrieval Evaluation**.  I hope you've noticed that the search results will differ (sometimes slightly, sometimes by a lot) depeding on which search method you used: `keyword_search` or `vector_search`.  As humans, it's fairly easy for us to determine whether the returned search results are relevant to the query that was submitted, (though even here there will be differing opinions on result relevance).  But how do we systematically determine which search method is better in general?  And how do we measure the relative performance of our retrieval system if we change one of it's parameters...for example, changing our embedding model? What about measuring system performance over time as more documents are added to our datastore?\n",
    "\n",
    "We need a way to evaluate our retrieval system, and this notebook will show you \"one way\" of doing that.  I say \"one way\" because there are many ways to approach this problem, and the method I'm showing you is not perfect (if anything it's a bit too conservative).  Ultimately, measuring retrieval performance is hard because it requires a lot of time and effort, and absent any user [click-data](https://en.wikipedia.org/wiki/Click_tracking), requires some form of data labeling.  With the advent of powerful generative LLMs the process of measuring retrieval performance has become much easier. Let's take a look at how that works.\n",
    "\n",
    "# Retrieval Evaluation - Process\n",
    "***\n",
    "Here's a high-level overview of how the Retrieval Evaluation process in this notebook works:\n",
    "\n",
    "1. Generate a \"golden dataset\" of query-context pairs.  I've already pre-generated multiple golden datasets for our course. I randomly selected 100 document chunks (contexts) from the Huberman Lab corpus and those chunks were then submitted to the `gpt-3.5-turbo` model which generated queries that can be answered by the context.  The output was 100 query-context pairs along with their associated doc_ids. \n",
    "   - **Baseline Assumptions**:\n",
    "     - The generated query-context pairs are, in fact, relevant to one another i.e. the query can be answered by the context that it's paired with\n",
    "     - The generated queries are simliar in style and length to the type of queries that end users would ask\n",
    "2. The golden dataset consists of three primary keys: `corpus`, `relevant_docs`, and `queries`\n",
    "     - The `corpus` is the original text context/chunk with it's associated `doc_id`\n",
    "     - The `queries` are the LLM generated queries, one (or more) for each entry in the `corpus`\n",
    "     - The `relevant_docs` is a simple lookup table linking the `corpus` docs to the generated `queries`\n",
    "3. We pass the golden dataset into a retrieval evluation function which does the following:\n",
    "   - Takes in a `retriever` arg (`WeaviateWCS`) and a few other configuration params\n",
    "   - Iterates over all queries in the golden dataset and retrieves search results for each query from the Weaviate datastore\n",
    "   - Extracts all `doc_id` values from the retrieved results\n",
    "   - Extracts the `doc_id` from the associated `relevant_docs` for each query\n",
    "   - Checks if the relevant doc_id is in the list of retrieved result doc_ids\n",
    "   - After all queries are completed a `hit_rate` score and `mrr` score are calculated for the entire golden dataset\n",
    "   - Writes results to an `eval_results` folder\n",
    "\n",
    "#### In a Nutshell\n",
    "Ulitmately, given a golden dataset consisting of queries, relevant docs, and their associated doc_ids, the `retrieval_evaluation` function is checking if the relevant doc_id is found in the list of retrieved results doc_ids, for each query.\n",
    "\n",
    "#### Problems with this Approach\n",
    "The problems with this approach are many, I'll cover a few here:\n",
    "- The **Assumptions** (see section 1 above) about the golden dataset must hold true.  Given that the pairs are generated by `gpt-3.5-turbo`, I think the first assumption will generally be true.  When reviewing the dataset I did find a few questions that were not answerable given the context, but for the most part they were.  The 2nd assumption though, is going to be dependent on your particular search use case.  I think for the purposes of this course, the questions generated are a decent reflection of how someone would query this dataset, and therefore do the job of measuring retriever performance.  But I would always check a real-world query distribution before using an approach like the one presented here.\n",
    "- This approach is conversative in that there is only \"one\" right answer.  Either the relevant `doc_id` is in the results list or it isn't.  In reality, there are going to be several documents that could potentially answer the generated query, but we have no way to account for these other relevant documents, unless of course, we want to manually add doc_ids to the golden dataset (and depending on your business case, you may actually want to do that).\n",
    "- We aren't measuring recall or precision because we aren't classifying other documents as \"negatives\".  As was just mentioned, the other documents in the results list may or may not be good matches, we just don't know.  Because we don't know, we can't really classify the other documents as \"negatives\".  So for this approach, we are measuring the [\"hit rate\"](https://uplimit.com/course/rag-applications/v2/module/retreival-evaluation#corise_clp66zqui003i2a777aldseor) which is simply a count of the number of times that we found a relevant `doc_id` match in the results list and [Mean Reciprocal Rank (MRR)](https://uplimit.com/course/rag-applications/v2/module/retreival-evaluation#corise_clp66zqui003j2a77u8lnrk5b).  We're using MRR instead of other metrics such as Mean Average Precision (MAP) because we are only looking at a [single relevant answer](https://stats.stackexchange.com/questions/127041/mean-average-precision-vs-mean-reciprocal-rank).  Hit rate is a good enough metric for determining if our retriever is retrieving quality results, and MRR will become more important later on when we add a Reranker to the mix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6a6f95-91c6-4a08-a3a0-2aafc2d56b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard library imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from typing import Any\n",
    "import time\n",
    "import os\n",
    "\n",
    "# utilities\n",
    "from tqdm import tqdm\n",
    "from rich import print\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "env = load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4522b4-5248-4936-af87-b0a2220bdb6d",
   "metadata": {},
   "source": [
    "# Assignment 1.3\n",
    "***\n",
    "#### Instructions:\n",
    "* Import the `/data/golden_datasets/golden_256.json` dataset using the `load_json` method of the FileIO Class\n",
    "* Instantiate a new WeaviateWCS client (Retriever) and set the `collection_name` of the Collection that you created in Notebook 2\n",
    "* Evaluate your retriever results using the `retrieval_evaluation` function\n",
    "* Submit your results in the form of a text file to Uplimit (the function autogenerates a report in the `dir_outpath` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9968b62-3814-40b9-bb2a-8c76968825c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num queries in Golden Dataset: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num queries in Golden Dataset: \u001b[1;36m100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.evaluation.retrieval_evaluation import calc_hit_rate_scores, calc_mrr_scores, record_results\n",
    "from src.database.weaviate_interface_v4 import WeaviateWCS\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "\n",
    "\n",
    "api_key = os.environ['WEAVIATE_API_KEY']\n",
    "url = os.environ['WEAVIATE_ENDPOINT']\n",
    "\n",
    "chunk_size = 512\n",
    "collection_name = \"Huberman_bge_base_en_v15_512\"\n",
    "golden_dataset_path = f'../data/golden_datasets/golden_{chunk_size}.json'\n",
    "\n",
    "### Load the QA dataset\n",
    "golden_dataset = FileIO.load_json(golden_dataset_path)\n",
    "\n",
    "### Instantiate Weaviate client and set Collection name\n",
    "model_path = 'BAAI/bge-base-en-v1.5'\n",
    "\n",
    "#instantiate client,\n",
    "retriever = WeaviateWCS(endpoint=url, api_key=api_key, model_name_or_path=model_path)\n",
    "retriever._client.is_live()\n",
    "\n",
    "# should see 100 queries\n",
    "print(f'Num queries in Golden Dataset: {len(golden_dataset[\"queries\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e13a3-9350-40d9-8bfe-0cd37f2ade2f",
   "metadata": {},
   "source": [
    "# Project 1: Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255bdabd-770e-4b12-8bd8-54e5cdc64f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_evaluation(dataset: dict,\n",
    "                         collection_name: str,\n",
    "                         retriever: WeaviateWCS,\n",
    "                         retrieve_limit: int=5,\n",
    "                         chunk_size: int=256,\n",
    "                         query_properties: list[str]=['content'],\n",
    "                         return_properties: list[str]=['doc_id', 'content'],\n",
    "                         dir_outpath: str='./eval_results',\n",
    "                         include_miss_info: bool=False\n",
    "                         ) -> dict[str, str|int|float]:\n",
    "    '''\n",
    "    Given a dataset and a retriever evaluate the performance of the retriever. Returns a dict of kw and vector\n",
    "    hit rates and mrr scores. If inlude_miss_info is True, will also return a list of kw and vector responses\n",
    "    and their associated queries that did not return a hit, for deeper analysis. Text file with results output\n",
    "    is automatically saved in the dir_outpath directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    dataset: dict\n",
    "        Dataset to be used for evaluation\n",
    "    collection_name: str\n",
    "        Name of Collection on Weaviate host to be used for retrieval\n",
    "    retriever: WeaviateWCS\n",
    "        WeaviateWCS object to be used for retrieval\n",
    "    retrieve_limit: int=5\n",
    "        Number of documents to retrieve from Weaviate host, increasing this value too high\n",
    "        will artificially inflate the hit rate score of your retriever.\n",
    "    chunk_size: int=256\n",
    "        Number of tokens used to chunk text. This value is purely for results\n",
    "        recording purposes and does not affect results.\n",
    "    return_properties: list[str]=['doc_id', 'content']\n",
    "        list of properties to be returned from Weaviate host for display in response\n",
    "    dir_outpath: str='./eval_results'\n",
    "        Directory path for saving results.  Directory will be created if it does not\n",
    "        already exist.\n",
    "    include_miss_info: bool=False\n",
    "        Option to include queries and their associated kw and vector response values\n",
    "        for queries that are \"total misses\"\n",
    "    '''\n",
    "\n",
    "    results_dict = {'n':retrieve_limit,\n",
    "                    'Retriever': retriever.model_name_or_path,\n",
    "                    'chunk_size': chunk_size,\n",
    "                    'query_props': query_properties,\n",
    "                    'kw_hit_rate': 0,\n",
    "                    'kw_mrr': 0,\n",
    "                    'vector_hit_rate': 0,\n",
    "                    'vector_mrr': 0,\n",
    "                    'total_misses': 0,\n",
    "                    'total_questions':0\n",
    "                    }\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    miss_info = []\n",
    "    for query_id, q in tqdm(dataset['queries'].items(), 'Queries'):\n",
    "        results_dict['total_questions'] += 1\n",
    "        hit = False\n",
    "\n",
    "        try:\n",
    "            kw_response = retriever.keyword_search(request=q, collection_name=collection_name, query_properties=query_properties,\n",
    "                                                   limit=retrieve_limit, return_properties=return_properties)\n",
    "            vector_response = retriever.vector_search(request=q, collection_name=collection_name,\n",
    "                                                   limit=retrieve_limit, return_properties=return_properties)\n",
    "\n",
    "            #collect doc_ids and position of doc_ids to check for document matches\n",
    "            kw_doc_ids = {result['doc_id']:i for i, result in enumerate(kw_response, 1)}\n",
    "            vector_doc_ids = {result['doc_id']:i for i, result in enumerate(vector_response, 1)}\n",
    "\n",
    "            #extract doc_id for scoring purposes\n",
    "            doc_id = dataset['relevant_docs'][query_id]\n",
    "\n",
    "            #increment hit_rate counters and mrr scores\n",
    "            if doc_id in kw_doc_ids:\n",
    "                results_dict['kw_hit_rate'] += 1\n",
    "                results_dict['kw_mrr'] += 1/kw_doc_ids[doc_id]\n",
    "                hit = True\n",
    "            if doc_id in vector_doc_ids:\n",
    "                results_dict['vector_hit_rate'] += 1\n",
    "                results_dict['vector_mrr'] += 1/vector_doc_ids[doc_id]\n",
    "                hit = True\n",
    "\n",
    "            # if no hits, let's capture that\n",
    "            if not hit:\n",
    "                results_dict['total_misses'] += 1\n",
    "                miss_info.append({'query': q, 'kw_response': kw_response, 'vector_response': vector_response})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "    #use raw counts to calculate final scores\n",
    "    calc_hit_rate_scores(results_dict, search_type=['kw', 'vector'])\n",
    "    calc_mrr_scores(results_dict, search_type=['kw', 'vector'])\n",
    "\n",
    "    end = time.perf_counter() - start\n",
    "    print(f'Total Processing Time: {round(end/60, 2)} minutes')\n",
    "    record_results(results_dict, chunk_size, dir_outpath=dir_outpath, as_text=True)\n",
    "\n",
    "    if include_miss_info:\n",
    "        return results_dict, miss_info\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5ed71-d150-4312-9d41-0b43ae43bd72",
   "metadata": {},
   "source": [
    "### Run evaluation over golden dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42568cd-d0e3-4640-8865-37b6dff80307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries: 100%|██████████| 100/100 [00:35<00:00,  2.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Processing Time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span> minutes\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Processing Time: \u001b[1;36m0.6\u001b[0m minutes\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = retrieval_evaluation(dataset=golden_dataset, collection_name=collection_name, retriever=retriever, chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979e79ec-46f4-4822-99a5-1b2b95209778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Retriever'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'BAAI/bge-base-en-v1.5'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'query_props'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'kw_hit_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'kw_mrr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.85</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'vector_hit_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.76</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'vector_mrr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.64</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'total_misses'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'total_questions'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'n'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "    \u001b[32m'Retriever'\u001b[0m: \u001b[32m'BAAI/bge-base-en-v1.5'\u001b[0m,\n",
       "    \u001b[32m'chunk_size'\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "    \u001b[32m'query_props'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'content'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'kw_hit_rate'\u001b[0m: \u001b[1;36m0.91\u001b[0m,\n",
       "    \u001b[32m'kw_mrr'\u001b[0m: \u001b[1;36m0.85\u001b[0m,\n",
       "    \u001b[32m'vector_hit_rate'\u001b[0m: \u001b[1;36m0.76\u001b[0m,\n",
       "    \u001b[32m'vector_mrr'\u001b[0m: \u001b[1;36m0.64\u001b[0m,\n",
       "    \u001b[32m'total_misses'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "    \u001b[32m'total_questions'\u001b[0m: \u001b[1;36m100\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61ced7-e641-4365-a821-f1b78e03d5c9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "***\n",
    "\n",
    "We now have a way of measuring the performance of our retrieval system.  This will allow you to make tweaks/changes to the system and then be able to objectively tell whether or not the tweak/change improved or degraded its performance.  Here are a few things to consider going forward:  \n",
    "\n",
    "- Keep in mind what the ulitmate goal of the system is that you are building.  For this course, we are trying to retrieve the most relevant documents as possible that will effectively address a user query, assuming the information is found within the corpus.  This means that we don't need pages and pages of relevant results, we actually only need the top 3-5, just enough to allow our Reader (the OpenAI LLM) to answer the user query.  This is an important point to be thinking about as you are making changes to the retrieval system.\n",
    "- Feel free to set the `include_miss_info` param to `True`.  Doing so will return a list of both the keyword and vector responses that did not contain the relevant `doc_id` (a \"total_miss\" means the `doc_id` was not present in either the `kw_doc_ids` or the `vector_doc_ids`).  Take a look at the style of the queries being asked and compare them with the returned responses.  Why are those responses being returned?  Are they close to the intent of the query?\n",
    "- Last but not least, you are now free to make changes to your system to improve the `hit_rate` and `mrr` scores.  If it were me, I'd start with switching out to a more performant [embedding model](https://huggingface.co/spaces/mteb/leaderboard).  There will be more opportunities to pick up some low hanging fruit, but we'll have to wait until the following week when hybrid search and Rerankers are introduced.  Whatever you do though, don't change params for the `SentenceSplitter` that you use for chunking the corpus.  Due to the way the golden dataset is derived, it's unfortunately dependent on those original `SentenceSplitter` settings remaining the same across evaluations. That is, of course, unless you want to build out your own golden dataset...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d63a1-b801-433b-b0db-3586e3b7bc59",
   "metadata": {},
   "source": [
    "# *** OPTIONAL: Chunk Size Evaluation ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e54d0-09ed-4784-a945-df39057c3025",
   "metadata": {},
   "source": [
    "In our initial Notebook we created a dataset with a chunk size of 256.  In order to evaluate the impact that chunk size has on retrieval for both search methods, it's a useful exercise to execute the `retrieval_evaluation` function on datasets of multiple chunk sizes.  In order to accomplish that follow these simple steps:\n",
    "- Bust out the `create_dataset` function that you created in Assignment 1.4.  Create datasets of chunks lengths **128** and **512**.  **Ensure that you set the `chunk_overlap` param to zero for each run.  Golden datasets of corresponding chunks lengths have already been created for you in the `data/golden_datasets` directory.\n",
    "- Index those datasets on Weaviate ensuring that you stick to the standard naming convention as discussed in Notebook 2 i.e `\"Huberman_minilm_<chunk_size>\"`\n",
    "- Evaluate results using the chunk sizes as a parameter, see example code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab3e31-de5b-4956-ab8f-d71d34c97d50",
   "metadata": {},
   "source": [
    "### Sample code for automated chunk_size evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6e3e24-a296-4b69-93cf-d6c261c04ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running test on model sentence-transformers/all-MiniLM-L6-v2, chunk size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running test on model sentence-transformers/all-MiniLM-L6-v2, chunk size \u001b[1;36m256\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zz/f8svd2n5583dff32w3y0k5zh0000gn/T/ipykernel_68634/3005919592.py:32: ResourceWarning: unclosed <ssl.SSLSocket fd=79, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.1.123', 53736), raddr=('34.149.137.116', 443)>\n",
      "  indexer = WeaviateIndexer(client=client)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating dataset embeddings<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating dataset embeddings\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset using chunk_size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 66/193 [02:43<05:25,  2.56s/it]/Users/fmc/miniconda3/envs/uplimit/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:304: ResourceWarning: unclosed <ssl.SSLSocket fd=81, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.1.123', 53739), raddr=('34.149.137.116', 443)>\n",
      "  encoding_dict[\"attention_mask\"].append(e.attention_mask)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/fmc/miniconda3/envs/uplimit/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:304: ResourceWarning: unclosed <ssl.SSLSocket fd=102, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.1.123', 53749), raddr=('34.149.137.116', 443)>\n",
      "  encoding_dict[\"attention_mask\"].append(e.attention_mask)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "100%|██████████| 193/193 [07:57<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time to process dataset of chunk_size (256): 8.15 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>DONE.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mDONE.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating Huberman_all_minilm_l6_v2_256 collection<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating Huberman_all_minilm_l6_v2_256 collection\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection \"Huberman_all_minilm_l6_v2_256\" created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>DONE.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mDONE.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Indexing the data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Indexing the data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23905/23905 [00:38<00:00, 619.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing finished in 1.21 minutes.\n",
      "Batch job completed with zero errors.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>DONE.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mDONE.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries: 100%|██████████| 100/100 [00:25<00:00,  3.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Processing Time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.43</span> minutes\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Processing Time: \u001b[1;36m0.43\u001b[0m minutes\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running test on model BAAI/bge-base-en-v1.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, chunk size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running test on model BAAI/bge-base-en-v1.\u001b[1;36m5\u001b[0m, chunk size \u001b[1;36m256\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zz/f8svd2n5583dff32w3y0k5zh0000gn/T/ipykernel_68634/3005919592.py:32: ResourceWarning: unclosed <ssl.SSLSocket fd=79, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.1.123', 53834), raddr=('34.149.137.116', 443)>\n",
      "  indexer = WeaviateIndexer(client=client)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating dataset embeddings<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating dataset embeddings\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset using chunk_size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 32/193 [04:59<25:05,  9.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuberman_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating dataset embeddings...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt35_txt_splitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_outpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcollection_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...DONE.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m collection...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Projects/vectorsearch-applications/notebooks/../src/embedding_utils.py:139\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(corpus, embedding_model, text_splitter, save_to_disk, file_outpath, unique_id_field, content_field, embedding_field, device)\u001b[0m\n\u001b[1;32m    134\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    136\u001b[0m content_splits \u001b[38;5;241m=\u001b[39m split_contents(\n\u001b[1;32m    137\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mcorpus, text_splitter\u001b[38;5;241m=\u001b[39mtext_splitter, content_field\u001b[38;5;241m=\u001b[39mcontent_field\n\u001b[1;32m    138\u001b[0m )\n\u001b[0;32m--> 139\u001b[0m text_vector_tuples \u001b[38;5;241m=\u001b[39m \u001b[43mencode_content_splits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m joined_docs \u001b[38;5;241m=\u001b[39m join_metadata(\n\u001b[1;32m    143\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mcorpus,\n\u001b[1;32m    144\u001b[0m     text_vector_list\u001b[38;5;241m=\u001b[39mtext_vector_tuples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     embedding_field\u001b[38;5;241m=\u001b[39membedding_field,\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_to_disk:\n",
      "File \u001b[0;32m~/Desktop/Projects/vectorsearch-applications/notebooks/../src/embedding_utils.py:73\u001b[0m, in \u001b[0;36mencode_content_splits\u001b[0;34m(content_splits, model, device)\u001b[0m\n\u001b[1;32m     71\u001b[0m text_vector_tuples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m splits \u001b[38;5;129;01min\u001b[39;00m tqdm(content_splits):\n\u001b[0;32m---> 73\u001b[0m     episode_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     text_vector_tuples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(splits, episode_vectors\u001b[38;5;241m.\u001b[39mtolist())))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text_vector_tuples\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:286\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    278\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 286\u001b[0m     mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uplimit/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.database.properties_template import properties\n",
    "from src.database.weaviate_interface_v4 import WeaviateIndexer\n",
    "from src.embedding_utils import create_dataset\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "\n",
    "all_results = []\n",
    "MODELS = [\n",
    "    'sentence-transformers/all-MiniLM-L6-v2', # Baseline for the course\n",
    "    'BAAI/bge-base-en-v1.5', # Higher ranked model on the MTEB\n",
    "    '567-labs/bge-base-en-v1.5-ft-quora-0.9' # Fine-tuned model using the Quora QA dataset\n",
    "]\n",
    "CHUNK_SIZES = [128, 256, 512]\n",
    "\n",
    "corpus_path = '../data/huberman_labs.json'\n",
    "corpus = FileIO.load_json(corpus_path)\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-0125')\n",
    "\n",
    "client = get_weaviate_client()\n",
    "all_collections = client.show_all_collections()\n",
    "\n",
    "for (model_name, chunk_size) in product(MODELS, CHUNK_SIZES):\n",
    "    print(f'Running test on model {model_name}, chunk size {chunk_size}...')\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    gpt35_txt_splitter = SentenceSplitter(chunk_size=chunk_size, tokenizer=encoding.encode, chunk_overlap=0)\n",
    "\n",
    "    client = get_weaviate_client(model_name_or_path=model_name)\n",
    "    indexer = WeaviateIndexer(client=client)\n",
    "\n",
    "    data_path = f'../data/golden_datasets/golden_{chunk_size}.json'\n",
    "    golden_dataset = FileIO().load_json(data_path)\n",
    "    base_model_name = model_name.split(\"/\")[1].lower().replace(\"-\", \"_\").replace(\".\", \"\")\n",
    "    collection_name = f'Huberman_{base_model_name}_{chunk_size}'\n",
    "\n",
    "    if collection_name not in all_collections:\n",
    "        dataset = create_dataset(corpus=corpus, text_splitter=gpt35_txt_splitter, embedding_model=model, save_to_disk=False, file_outpath=f'../data/{collection_name}.parquet')\n",
    "        client.create_collection(collection_name=collection_name, properties=properties, description='Huberman Labs: 193 full-length transcripts')\n",
    "        batch_object = indexer.batch_index_data(data=dataset, collection_name=collection_name, vector_property=\"content_embedding\", unique_id_field=\"doc_id\")\n",
    "\n",
    "    results = retrieval_evaluation(golden_dataset, collection_name, client, query_properties=['content'], chunk_size=chunk_size)\n",
    "    all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58382e8b-8c53-4b22-989e-05fe18299ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uplimit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
